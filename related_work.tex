\section{Related Work}
\label{sec-related-work}
\subsection{Shoulder Surfing}
With the arrival of the information era, privacy issues are becoming increasingly prominent. Smartphone screen privacy, the concern of our smartphones being observed by strangers in public areas, or shoulder surfing, have been studied heavily in recent years. Surveys of shoulder surfing provides evidence of this behavior in real world \cite{eiband2017understanding} \cite{goucher2011look}, often more efficient than expected\cite{kwon2013covert}. Various techniques and systems have been designed to mitigate this threat. Some systems sense malicious passers-by and hide information \cite{brudy2014anyone} or warn the user\cite{saad2018communicating}; others modify the user interfaces to create honeypots (for passwords)\cite{chakraborty2014tag}, confuse unauthorized parties\cite{wiedenbeck2006design}, or making the interactions invisible\cite{kumar2007reducing} or unreadable from a distance\cite{Chun2019Keep}. Most of the works designing defenses against shoulder surfing assume that the attacker is a casual passer-by, taking occasional peeks with his/hers naked eye, as is the case most of the time\cite{eiband2017understanding}. However, it is the malicious ones, although few, that can utilize the information (passwords, business correspondence, etc.) they obtained to do real harm. On the other hand, the threat model of tool-assisted shoulder surfing is also studied. Schaub, et, al. observed the susceptibility of shoulder surfing from the naked eye\cite{schaub2012password}. Maggi, et al. designed an automatic shoulder surfing threat model, observing the target smartphone with a camera\cite{maggi2011poster}, but without the help of SR techniques, this method can only function when the attacker is standing at close range, which is a barely practical scenario. With the development of smartphone camera, processing ability, and SR technology over the years, we propose a stronger shoulder surfing system in which attackers can successfully obtain information while keeping a distance to avoid suspicion, and prove that this threat is eminent in our daily life.

\subsection{Super Resolution}
Image super-resolution is the process of reconstructing an image with a higher spatial resolution. Single image super-resolution techniques accept a single low-res image as input, and based on its structural pattern, self-similarity \cite{suetake2008image}, or previous knowledge of the genre of the image, deduces missing information and reconstructs the missing pixels to form a sharp, high-res image. On the other hand, multiple image super-resolution techniques work on a set of pictures on the same scene, e.g. multiple snapshots captured by the camera of a smartphone, successive images from a satellite, or adjacent frames on a video clip. Although they picture the same scene and are mostly identical to each other, those low-res pictures can be viewed as replicas of a certain high-res scene with random pixels blurred and removed, and multiple image super-resolution algorithms reconstruct the high-res scene by merging these incomplete information sources, often exhibiting better performance than single image super-resolution.

As mentioned above, multiple image super-resolution algorithms are based on the assumption that all input images are reflections of the same scene. It can be assumed that there exists a high-res image H, and each input low-res image Li is a version of an aliased, blurred, downsampled and noised H. By modelling the alias, blur and noise effects, and comparing these low-res inputs, we can grasp an estimation of those degrading effects and neutralize them, and the output of our super-resolution algorithm will be a high-res image H’ with the most possibility to single-handedly generate all the input images Li via the aliasing, blurring, downsampling and noising processes. 

Over the last two decades a variety of techniques have been proposed for the multiple image super-resolution problem. Early works focus on the analysis of the images on frequency or special domains, focusing on their pixel level differences and merging their information. Spatial domain methods mostly work on the interpolation approach, trying to insert new pixels between pixels of the low-res images. One of the well-known methods in the spatial domain is the Shift-Add algorithm \cite{farsiu2003robust}, which functions by maximizing the pixel-wise possibility of high-res image generating low-res images with gradient decent methods. On the other hand, frequency domain approaches focus on the Fourier transform of the images, and reconstruct high-res images based on patterns of the images in the frequency domain. Downsampling processes remove high-frequency components of the images, preserving low-frequency components; Noises often exists in a certain frequency band and can thus be identified and removed. By assuming that the high-res scene is band-limited, with CFT and DFT transformations, frequency domain algorithms can reconstruct high-frequency components from patterns of the low-frequency components, removing bands of noise and blur effects at the same time. A great variety of techniques have been proposed, however, due to the nature of the problem, these algorithms are all tailored for their application: natural photos, scanned text, satellite imaging, biometrics, etc. and achieve best performance only in their own field.

More recent works of Super Resolution are often based on deep learning approaches. In 2016, SRCNN \cite{dong2015image} was developed from CNN replacing pooling layers with upsampling layers, achieving notably better performance than previous approaches. This work focuses on single image super-resolution, as the neural network accepts a single image as input. As Generative Adversarial Networks (GAN) were proposed, the network good at constructing images pleasant to the human eye was also introduced into the domain of super resolution \cite{ledig2017photo}. These networks generate photo-realistic images, with less artifacts and more genuine-looking details pleasing to the human eye, though not always accurate against the real scene. Note that super resolution is an ill-posed problem, that is, we do not have a ‘ground truth’ to begin with, and traditional evaluation methods, e.g. mean square error (MSE) might not be suitable according to the application. Although SRGAN might score slightly lower on MSE, it can reconstruct more high-frequency and realistic details and score much higher on the mean opinion score (MOS). However, when published, the above works were limited to single input images. Because of increasing complexity and training difficulty, we cannot directly modify these super resolution neural networks to accept multiple images as input. There are also various works adapting neural networks to perform multi-image SR tasks. The most common variation is video SR\cite{shi2016real, kappeler2016video}, accepting complete video clips as input, and functioning on the similarity between frames. The sequentiality and consistency between adjacent frames makes it easy for the convolution networks to comprehend, and by modifying the 2d convolution layers to 3d convolution\cite{caballero2017real}, modifying the dataflow to merge neighboring frames among the network layers\cite{huang2017video}, or recurrently processing the frames under the guidance of the output of the previous frame\cite{sajjadi2018frame}, the task can be completed easily. Other works face image groups without consistency or sequential information, e.g. satellite images. To achieve appreciable results, most works choose hybrid methods, solving the multi-image SR problem with multiple single-image SR procedures. They commonly function by merging the results of single image SR algorithms to efficiently utilize input information\cite{kawulok2019deep}, or building a multi-image network to create a course view and support it with single image SR networks\cite{8834937}. These methods expect at least some degree of information to be extracted from each of the single frames with single image SR methods.

However, the methods mentioned above are not suitable for our application and we need a novel approach. The blurriness of our photos exceeds the processing ability of non-learning methods; furthermore, these photos are extremely blurred, defocused and full of noise so that few information can be extracted from them via single image SR methods; The adjacent frames display drastically different blurring patterns so that they lack consistency between adjacent frames. Only from an overall view of all the images can we distinguish noise from facts, and we need a network that can comprehend the similarities and differences between each frame to recover the buried information, and indeed, building an end-to-end multi-image SR network without assuming sequential consistency between frames is still a very much underexplored field of study, and our work aims at filling that gap.

